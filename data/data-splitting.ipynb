{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53c966e4",
   "metadata": {},
   "source": [
    "# Sycophancy datasets\n",
    "\n",
    "This is a collection of open datasets that might be useful to study sychopancy on LLMs. There datasets have been used by other reseach studies to evaluate different capabilities. Some datasets are directly priented towards an evaluation of sychopancy, while others can evaluate sychopancy indirectly.\n",
    "\n",
    "For every datasets there is included:\n",
    " - Name of the dataset\n",
    " - Link and further description\n",
    " - Research studies where the dataset has been used\n",
    " - Brief note on how the dataset was used\n",
    " - Code to load the dataset\n",
    " - Dataset sample\n",
    " \n",
    "The datasets are ranked into different tiers, where a higher tier means the dataset is more well suited to measure sycophancy.\n",
    "\n",
    "In case the dataset is contained in a repository, the dataset is downloaded directly from the link in the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5380a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import json\n",
    "import glob\n",
    "import string\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb89504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_idxs(data):\n",
    "    n_train = 200\n",
    "    n_val = 100\n",
    "    n_test = 100\n",
    "    idx = np.random.choice(len(data), n_train+n_val+n_test)\n",
    "\n",
    "    idx_train = idx[:n_train]\n",
    "    idx_val = idx[n_train:n_train+n_val]\n",
    "    idx_test = idx[n_train+n_val:]\n",
    "    \n",
    "    return idx_train, idx_val, idx_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706164cc",
   "metadata": {},
   "source": [
    "## TruthfulQA\n",
    "link: https://huggingface.co/datasets/truthful_qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a17fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### There are available some educated roles I generated to preppend to the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef00a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import role, educated_roles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a5bfa2",
   "metadata": {},
   "source": [
    "### generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14c3c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "truthful_qa_g = load_dataset(\"truthful_qa\", 'generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e5ef7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_qa_g = truthful_qa_g['validation'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c780d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "truthful_qa_g_types = []\n",
    "for f in features_qa_g.keys():\n",
    "    print(f, ':', truthful_qa_g['validation'][f][0], end='\\n\\n')\n",
    "    \n",
    "print('-'*20)\n",
    "for d in truthful_qa_g['validation']:\n",
    "    truthful_qa_g_types.append(d['type'])\n",
    "print('Types:', set(truthful_qa_g_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dc354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train, idx_val, idx_test = generate_idxs(truthful_qa_g['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6697d39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = [truthful_qa_g['validation'][int(i)] for i in idx_train]\n",
    "data_val = [truthful_qa_g['validation'][int(i)] for i in idx_val]\n",
    "data_test = [truthful_qa_g['validation'][int(i)] for i in idx_test]\n",
    "\n",
    "data_splits = {\n",
    "        'train': data_train,\n",
    "        'validation': data_val,\n",
    "        'test': data_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86352e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'TrutfulQA/truthfulqa-generation-splitted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e2b1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data_splitted/{file_name}.txt', 'w') as file:\n",
    "    file.write(str(data_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65fa46b",
   "metadata": {},
   "source": [
    "### MQA\n",
    "\n",
    "`mc2_targets` contain multiple possible correct anwsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699d720",
   "metadata": {},
   "outputs": [],
   "source": [
    "truthful_qa_mcq = load_dataset(\"truthful_qa\", 'multiple_choice')\n",
    "features_qa_mcq = truthful_qa_mcq['validation'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6c2914",
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in features_qa_mcq.keys():\n",
    "    print(f, ':', truthful_qa_mcq['validation'][f][100], end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e4ff8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train, idx_val, idx_test = generate_idxs(truthful_qa_mcq['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f804fba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = [truthful_qa_mcq['validation'][int(i)] for i in idx_train]\n",
    "data_val = [truthful_qa_mcq['validation'][int(i)] for i in idx_val]\n",
    "data_test = [truthful_qa_mcq['validation'][int(i)] for i in idx_test]\n",
    "\n",
    "data_splits = {\n",
    "        'train': data_train,\n",
    "        'validation': data_val,\n",
    "        'test': data_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34bde09",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'TrutfulQA/truthfulqa-mqa-splitted'\n",
    "\n",
    "with open(f'data_splitted/{file_name}.txt', 'w') as file:\n",
    "    file.write(str(data_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54a72e0",
   "metadata": {},
   "source": [
    "## SVAMP\n",
    "\n",
    "used in Wei et al. 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69a436f",
   "metadata": {},
   "outputs": [],
   "source": [
    "svamp = load_dataset(\"ChilleD/SVAMP\")\n",
    "svamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37068b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train:', len(svamp['train']))\n",
    "print('test:', len(svamp['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b95562",
   "metadata": {},
   "outputs": [],
   "source": [
    "svamp_keys = svamp['train'].features.keys()\n",
    "svamp_types = [] \n",
    "for k in svamp_keys:\n",
    "    print(k, ':', svamp['train'][0][k])\n",
    "    print()\n",
    "print('-'*20)\n",
    "for d in svamp['train']:\n",
    "    svamp_types.append(d['Type'])\n",
    "print('Types:', set(svamp_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad95bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train, idx_val, idx_test = generate_idxs(truthful_qa_mcq['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf15418",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = [svamp['train'][int(i)] for i in range(200)]\n",
    "data_val = [svamp['test'][i] for i in range(100)]\n",
    "data_test = [svamp['test'][i] for i in range(100, 200)]\n",
    "\n",
    "data_splits = {\n",
    "        'train': data_train,\n",
    "        'validation': data_val,\n",
    "        'test': data_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8e2b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'SV'\n",
    "\n",
    "with open(f'data_splitted/{file_name}.txt', 'w') as file:\n",
    "    file.write(str(data_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b7ab0c",
   "metadata": {},
   "source": [
    "## GSM8K\n",
    "\n",
    "used in Wei et al. 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f49aaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k = load_dataset(\"gsm8k\", 'main')\n",
    "print('train:', len(gsm8k['train']))\n",
    "print('test:', len(gsm8k['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f100bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_keys = gsm8k['train'].features.keys()\n",
    "gsm8k_types = [] \n",
    "for k in gsm8k_keys:\n",
    "    print(k, ':', gsm8k['train'][0][k])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9951a83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = gsm8k['train']\n",
    "idx_train, idx_val, idx_test = generate_idxs(data)\n",
    "\n",
    "data_train = [data[int(i)] for i in idx_train]\n",
    "data_val = [data[int(i)] for i in idx_val]\n",
    "data_test = [data[int(i)] for i in idx_test]\n",
    "\n",
    "data_splits = {\n",
    "        'train': data_train,\n",
    "        'validation': data_val,\n",
    "        'test': data_test\n",
    "    }\n",
    "\n",
    "file_name = 'GSM8K/gsm8k'\n",
    "\n",
    "with open(f'data_splitted/{file_name}.txt', 'w') as file:\n",
    "    file.write(str(data_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32cfa88",
   "metadata": {},
   "source": [
    "## MathQA\n",
    "\n",
    "used in Wei et al. 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5024bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_qa = load_dataset(\"math_qa\", 'main')\n",
    "print('train:', len(math_qa['train']))\n",
    "print('validation:', len(math_qa['validation']))\n",
    "print('test:', len(math_qa['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c963ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_qa_keys = math_qa['train'].features.keys()\n",
    "math_qa_types = [] \n",
    "for k in math_qa_keys:\n",
    "    print(k, ':', math_qa['train'][0][k])\n",
    "    print()\n",
    "print('-'*20)\n",
    "for d in math_qa['train']:\n",
    "    math_qa_types.append(d['category'])\n",
    "print('category:', set(math_qa_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5d77bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = math_qa['train']\n",
    "idx_train, idx_val, idx_test = generate_idxs(data)\n",
    "\n",
    "data_train = [data[int(i)] for i in idx_train]\n",
    "data_val = [data[int(i)] for i in idx_val]\n",
    "data_test = [data[int(i)] for i in idx_test]\n",
    "\n",
    "data_splits = {\n",
    "        'train': data_train,\n",
    "        'validation': data_val,\n",
    "        'test': data_test\n",
    "    }\n",
    "\n",
    "file_name = 'MathQA/math-qa'\n",
    "\n",
    "with open(f'data_splitted/{file_name}.txt', 'w') as file:\n",
    "    file.write(str(data_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457d47c1",
   "metadata": {},
   "source": [
    "## AQuA\n",
    "\n",
    "used in Wei et al. 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e768ee9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "aqua_rat = load_dataset(\"aqua_rat\", 'raw') # also tokenized version\n",
    "print('train:', len(aqua_rat['train']))\n",
    "print('validation:', len(aqua_rat['validation']))\n",
    "print('test:', len(aqua_rat['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad037cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "aqua_rat_keys = aqua_rat['train'].features.keys()\n",
    "aqua_rat_types = [] \n",
    "for k in aqua_rat_keys:\n",
    "    print(k, ':', aqua_rat['train'][0][k])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7220ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = aqua_rat['train']\n",
    "idx_train, idx_val, idx_test = generate_idxs(data)\n",
    "\n",
    "data_train = [data[int(i)] for i in idx_train]\n",
    "data_val = [data[int(i)] for i in idx_val]\n",
    "data_test = [data[int(i)] for i in idx_test]\n",
    "\n",
    "data_splits = {\n",
    "        'train': data_train,\n",
    "        'validation': data_val,\n",
    "        'test': data_test\n",
    "    }\n",
    "\n",
    "file_name = 'AQuA/aqua'\n",
    "\n",
    "with open(f'data_splitted/{file_name}.txt', 'w') as file:\n",
    "    file.write(str(data_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5dde29",
   "metadata": {},
   "source": [
    "## RepE evals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66dd5bb",
   "metadata": {},
   "source": [
    "- emotions\n",
    "- facts\n",
    "- memorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929cfa42",
   "metadata": {},
   "source": [
    "## Anthropic evals\n",
    "\n",
    "Taken from `https://github.com/anthropics/evals`\n",
    "\n",
    "*Note*: looks like the dataset is WRONG ON HUGGINGFACE. In HF NLP and philapers are the same file. and Download it from github.\n",
    "\n",
    "**A/B choice**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bfe32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = [\n",
    "  'sycophancy_on_nlp_survey.jsonl',\n",
    "  'sycophancy_on_philpapers2020.jsonl',\n",
    "  'sycophancy_on_political_typology_quiz.jsonl'\n",
    "]\n",
    "\n",
    "all_data = []\n",
    "for item in DATASETS:\n",
    "    print(item)\n",
    "    \n",
    "    url = f\"https://github.com/anthropics/evals/raw/main/sycophancy/{item}\"\n",
    "    r = requests.get(url).text\n",
    "    data = [json.loads(l) for l in r.split(\"\\n\") if l != '']\n",
    "    #print(data)\n",
    "    all_data.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2935473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928b9214",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\n",
    "    'nlp_survey',\n",
    "    'philpapers2020',\n",
    "    'political_typology_quiz'\n",
    "]\n",
    "for name, data in zip(names, all_data):\n",
    "    idx_train, idx_val, idx_test = generate_idxs(data)\n",
    "\n",
    "    data_train = [data[int(i)] for i in idx_train]\n",
    "    data_val = [data[int(i)] for i in idx_val]\n",
    "    data_test = [data[int(i)] for i in idx_test]\n",
    "\n",
    "    data_splits = {\n",
    "            'train': data_train,\n",
    "            'validation': data_val,\n",
    "            'test': data_test\n",
    "        }\n",
    "\n",
    "    file_name = f'anthropic-evals/{name}'\n",
    "\n",
    "    with open(f'data_splitted/{file_name}.txt', 'w') as file:\n",
    "        file.write(str(data_splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6f1cf5",
   "metadata": {},
   "source": [
    "#### NLP survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02ddb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('lenght:', len(all_data[0]))\n",
    "nlp_keys = all_data[0][0].keys()\n",
    "for k in nlp_keys:\n",
    "    print(k, ':', all_data[0][0][k])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914e5411",
   "metadata": {},
   "source": [
    "#### Philpapers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed66c1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('lenght:', len(all_data[1]))\n",
    "pp_keys = all_data[1][0].keys()\n",
    "for k in pp_keys:\n",
    "    print(k, ':', all_data[1][0][k])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc9a001",
   "metadata": {},
   "source": [
    "#### Political typology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1880f207",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('lenght:', len(all_data[2]))\n",
    "pp_keys = all_data[2][0].keys()\n",
    "for k in pp_keys:\n",
    "    print(k, ':', all_data[2][0][k])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b511d12b",
   "metadata": {},
   "source": [
    "### Nina's sycophancy\n",
    "\n",
    "Get the repo from here: `git clone https://github.com/nrimsky/LM-exp`\n",
    "\n",
    "Taken from TruthfulQA\n",
    "\n",
    "**A/B choice**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8765cdb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('LM-exp/datasets/sycophancy_2/sycophancy_agree_disagree_data.json'))\n",
    "all_data = []\n",
    "for d in data:\n",
    "    all_data.append({\n",
    "        'sycophantic_text': 'Question: ' + d['question'] + '\\nAnswer:' + d['answer_matching_behavior'],\n",
    "        'non_sycophantic_text': 'Question: ' + d['question'] + '\\nAnswer:' + d['answer_not_matching_behavior']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9f47f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_data[0]['sycophantic_text'])\n",
    "print()\n",
    "print(all_data[0]['non_sycophantic_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cccffda4",
   "metadata": {},
   "source": [
    "- NLP survey\n",
    "- PhilPapers2020\n",
    "- Political Typology\n",
    "- Nina's dataset (did not found the source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8a5df",
   "metadata": {},
   "source": [
    "## Sycophancy evals\n",
    "link: https://github.com/meg-tong/sycophancy-eval/\n",
    "\n",
    "Used in the paper TOWARDS UNDERSTANDING SYCOPHANCY IN LANGUAGE MODELS (Sharma 2023)\n",
    "\n",
    "- `are_you_sure.jsonl` corresponds to 3.2 AI ASSISTANTS CAN BE EASILY SWAYED\n",
    "- `answer.jsonl` corresponds to 3.3 AI ASSISTANTS CAN PROVIDE ANSWERS THAT CONFORM TO USER BELIEFS\n",
    "- `feedback.jsonl`corresponds to 3.4 AI ASSISTANT RESPONSES SOMETIMES MIMIC USER MISTAKES\n",
    "\n",
    "For the test 3.1 AI ASSISTANTS CAN GIVE BIASED FEEDBACK the following datasets are used: (i) math solutions from MATH (Hendrycks et al., 2021b); (ii) model-generated arguments; and (iii) model-generated poems. these are not included in this dataset.\n",
    "\n",
    "The datasets used to compose sycophancy evals are the following:\n",
    "\n",
    "- `are_you_sure.jsonl : {'mmlu_mc_cot', 'truthful_qa', 'aqua_mc', 'math_mc_cot', 'truthful_qa_mc', 'trivia_qa'}`\n",
    "- `answer.jsonl : {'trivia_qa', 'truthful_qa'}`\n",
    "- `feedback.jsonl : {'poems', 'math', 'arguments'}`\n",
    "\n",
    "MMLU, MATH, and AQuA are missing from the `are_you_sure` dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfd4298",
   "metadata": {},
   "outputs": [],
   "source": [
    "items = [\n",
    "    'are_you_sure.jsonl',\n",
    "    'answer.jsonl',\n",
    "    'feedback.jsonl'\n",
    "]\n",
    "# aarons repo: https://github.com/ascher8/sycophancy-eval/tree/main/datasets\n",
    "\n",
    "names = [\n",
    "    'are_you_sure',\n",
    "    'answer',\n",
    "    'feedback'\n",
    "]\n",
    "\n",
    "for name, item in zip(names, items):\n",
    "    url = f\"https://raw.githubusercontent.com/meg-tong/sycophancy-eval/main/datasets/{item}\"\n",
    "    r = requests.get(url).text\n",
    "    data = [json.loads(l) for l in r.split(\"\\n\") if l != '']\n",
    "    datasets_used = []\n",
    "    for d in data:\n",
    "        datasets_used.append(d['base']['dataset'])\n",
    "    print(item, ':', set(datasets_used))\n",
    "    #print(d)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e05a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train, idx_val, idx_test = generate_idxs(data)\n",
    "\n",
    "data_train = [data[int(i)] for i in idx_train]\n",
    "data_val = [data[int(i)] for i in idx_val]\n",
    "data_test = [data[int(i)] for i in idx_test]\n",
    "\n",
    "data_splits = {\n",
    "        'train': data_train,\n",
    "        'validation': data_val,\n",
    "        'test': data_test\n",
    "    }\n",
    "\n",
    "file_name = f'feedback/feedback'\n",
    "\n",
    "with open(f'data_splitted/{file_name}.txt', 'w') as file:\n",
    "    file.write(str(data_splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f26da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/mimicry.jsonl', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "data = [json.loads(line) for line in lines]\n",
    "datasets_used = []\n",
    "for d in data:\n",
    "    datasets_used.append(d['base']['attribution'])\n",
    "print(item, ':', set(datasets_used))\n",
    "print()\n",
    "# mimicry dataset just contains poems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
