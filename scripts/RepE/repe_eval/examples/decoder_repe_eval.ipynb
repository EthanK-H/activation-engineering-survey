{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ddb8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d4f2f3-0e04-46ba-a7a4-ce723abe9c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from itertools import islice\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Local task dataloaders\n",
    "sys.path.append('../')\n",
    "sys.path.append('../../')\n",
    "from tasks import task_dataset\n",
    "\n",
    "# load repe module\n",
    "from repe import repe_pipeline_registry\n",
    "repe_pipeline_registry()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e985c96-4df2-42d0-82d1-14cd4fbec28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             #device_map=\"cuda\",\n",
    "                                             token=os.getenv('HF_TOKEN')).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11384f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_fast_tokenizer = \"LlamaForCausalLM\" not in model.config.architectures\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path,\n",
    "                                          use_fast=use_fast_tokenizer,\n",
    "                                          padding_side=\"left\",\n",
    "                                          legacy=True, ## check this! It does not work with False\n",
    "                                          token=os.getenv('HF_TOKEN'))\n",
    "tokenizer.pad_token_id = 0 if tokenizer.pad_token_id is None else tokenizer.pad_token_id\n",
    "tokenizer.bos_token_id = 1\n",
    "\n",
    "rep_pipeline =  pipeline(\"rep-reading\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c186035",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f097707c-5256-47ae-9c6d-39f79e8ea6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_length = 2048\n",
    "\n",
    "rep_token = -1\n",
    "hidden_layers = list(range(-1, -model.config.num_hidden_layers, -1))\n",
    "n_difference = 1\n",
    "direction_method = 'pca'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f3042e-2667-46c4-89bf-52e0fafe0a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tasks and #shots here\n",
    "\n",
    "# task, ntrain = 'obqa', 5\n",
    "# task, ntrain = 'csqa', 7\n",
    "task, ntrain = 'arc_challenge', 25\n",
    "# task, ntrain = 'race', 3\n",
    "\n",
    "\n",
    "dataset = task_dataset(task)(ntrain=ntrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5d269b-6da7-47b4-abf8-f538923f594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an unsupervised LAT PCA representation\n",
    "direction_finder_kwargs= {\"n_components\": 1}\n",
    "rep_reader = rep_pipeline.get_directions(\n",
    "    dataset['train']['data'], \n",
    "    rep_token=rep_token, \n",
    "    hidden_layers=hidden_layers, \n",
    "    n_difference=n_difference, \n",
    "    train_labels=dataset['train']['labels'], \n",
    "    direction_method=direction_method,\n",
    "    direction_finder_kwargs=direction_finder_kwargs,\n",
    "    batch_size=batch_size,\n",
    "    max_length=max_length,\n",
    "    padding=\"longest\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257ab291-2bd7-4c98-85fd-e6b67eeb263e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Eval validation\n",
    "results_val = {layer: {} for layer in hidden_layers}\n",
    "labels = dataset['val']['labels']\n",
    "H_tests = rep_pipeline(dataset['val']['data'],\n",
    "                    rep_token=rep_token, \n",
    "                    hidden_layers=hidden_layers, \n",
    "                    rep_reader=rep_reader,\n",
    "                    batch_size=8,\n",
    "                    max_length=2048,\n",
    "                    padding=\"longest\")\n",
    "\n",
    "for layer in hidden_layers:\n",
    "    H_test = [H[layer] for H in H_tests] \n",
    "    unflattened_H_tests = [list(islice(H_test, sum(len(c) for c in labels[:i]), sum(len(c) for c in labels[:i+1]))) for i in range(len(labels))]\n",
    "\n",
    "    sign = rep_reader.direction_signs[layer]\n",
    "    eval_func = np.argmin if sign == -1 else np.argmax\n",
    "    cors = np.mean([labels[i].index(1) == eval_func(H) for i, H in enumerate(unflattened_H_tests)])\n",
    "\n",
    "    results_val[layer] = cors\n",
    "    \n",
    "    print(f\"{layer} : {cors}\")\n",
    "    print(\"=====\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a087af5-45a9-4a74-921f-d1b068ea1113",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Eval Test\n",
    "results_test = {layer: {} for layer in hidden_layers}\n",
    "labels = dataset['test']['labels']\n",
    "H_tests = rep_pipeline(dataset['test']['data'], \n",
    "                    rep_token=rep_token, \n",
    "                    hidden_layers=hidden_layers, \n",
    "                    rep_reader=rep_reader,\n",
    "                    batch_size=8,\n",
    "                    max_length=2048,\n",
    "                    padding=\"longest\")\n",
    "\n",
    "for layer in hidden_layers:\n",
    "    H_test = [H[layer] for H in H_tests] \n",
    "    unflattened_H_tests = [list(islice(H_test, sum(len(c) for c in labels[:i]), sum(len(c) for c in labels[:i+1]))) for i in range(len(labels))]\n",
    "\n",
    "    sign = rep_reader.direction_signs[layer]\n",
    "    eval_func = np.argmin if sign == -1 else np.argmax\n",
    "    cors = np.mean([labels[i].index(1) == eval_func(H) for i, H in enumerate(unflattened_H_tests)])\n",
    "\n",
    "    results_test[layer] = cors\n",
    "    \n",
    "    print(f\"{layer} : {cors}\")\n",
    "    print(\"=====\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bcd823-6a3a-4c67-b752-2fd890b4df80",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(results_val.keys())\n",
    "y_val = [results_val[layer] for layer in hidden_layers]\n",
    "y_test = [results_test[layer] for layer in hidden_layers]\n",
    "\n",
    "\n",
    "plt.plot(x, y_val, label=\"Dev\")\n",
    "plt.plot(x, y_test, label=\"Test\")\n",
    "\n",
    "plt.title(f\"{task} Acc by Layer\")\n",
    "plt.xlabel(\"Layer\")\n",
    "plt.ylabel(\"Acc\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
