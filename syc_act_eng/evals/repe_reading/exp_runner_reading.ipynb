{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reading_vec_dataset_name = \"sycophancy_function_facts\" # dataset to get reading vectors from\n",
    "use_play_reading_vec_dataset = False\n",
    "\n",
    "eval_dataset_name = \"feedback-math\" # dataset to evaluate model on # OPTIONS=[\"anthropic_nlp\", \"feedback-math\"]\n",
    "\n",
    "model_name_or_path = \"mistralai/Mistral-7B-Instruct-v0.1\" # model to use\n",
    "eval_n_samples = 20 # number of samples to use for evaluation\n",
    "\n",
    "load_model = False # set to false for debugging dataset\n",
    "\n",
    "cache_dir = '/workspace/model_cache' # where to save and load model cache\n",
    "token = \"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\" # huggingface token\n",
    "\n",
    "reading_batch_size = 8 # batch size for evaluation (keep low to avoid memory issues)\n",
    "eval_batch_size = 8 # batch size for evaluation\n",
    "coeff = 2.0 # reading vector coefficient\n",
    "max_new_tokens = 10 # maximum number of tokens for model to generate\n",
    "layer_id = list(range(-5, -18, -1)) # layers to apply reading vectors\n",
    "\n",
    "do_wandb_track = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "from syc_act_eng.evals.repe_reading.exp_runner import RepeReadingEval\n",
    "from syc_act_eng.utils import print_cuda_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cuda_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if load_model:\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, torch_dtype=torch.float16, device_map=\"auto\", use_auth_token=token, cache_dir=cache_dir)\n",
    "    use_fast_tokenizer = \"LlamaForCausalLM\" not in model.config.architectures\n",
    "else:\n",
    "    use_fast_tokenizer = True\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=use_fast_tokenizer, padding_side=\"left\", legacy=False)\n",
    "tokenizer.pad_token_id = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_cuda_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Exp Runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_runner = RepeReadingEval(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    model_name_or_path=model_name_or_path,\n",
    "    reading_vec_dataset_name=reading_vec_dataset_name,\n",
    "    eval_dataset_name=eval_dataset_name,\n",
    "    eval_n_samples=eval_n_samples,\n",
    "    reading_batch_size=reading_batch_size,\n",
    "    eval_batch_size=eval_batch_size,\n",
    "    coeff=coeff,\n",
    "    max_new_tokens=max_new_tokens,\n",
    "    layer_id=layer_id,\n",
    "    do_wandb_track=do_wandb_track\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_runner.init_reading_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_runner.eval(eval_dataset_name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
